
## Preliminary: a refresh



## Neural net basics

## Natural Language Processing

# LLM basic

**High level resources**

Here a list of resources that can give a great grasp of inner mechanism of a transformer. I add also different resources about self-attention.

* [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Jay Alammar. The most famous and illustrated explanation of the transformer.
* [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/) - Jay Alammar. GPT-2 is the grandfather of current LLMs (autoregressive model), this detailed blog post explain in details the model
* [Visual intro to Transformers](https://www.youtube.com/watch?v=wjZofJX0v4M&t=187s) - 3Blue1Brown. Another great resource about an introduction on transformer in a video
* [LLM Visualization](https://bbycroft.net/llm)  Brendan Bycroft. 3D visualization to dig inside the transformer
* [nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) - Andrej Karpathy. Karpathy explain in this video how to implement GPT model from scratch (very useful at the programming side).
* [Coding the Self-Attention Mechanism of LLMS From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html) - Sebastian Raschka. Great blog post where is explained how to code self-attention (and cross-attention) in PyTorch
* [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) - Lilian Weng. A formal description of attention
* [Decoding Strategies in LLMs](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html). An introduction to the different strategies for decoding. 

**Theroretical resources**

A list of resources about a more formal introduction on transformers and attention. I strongly suggest these resources to whom want to dig and better understand in details.

* [Transformers and Large Language Models](https://web.stanford.edu/~jurafsky/slp3/10.pdf) - Dan Jurafsky and James H. Martin. This chapter of Speech and Language Processing describes in details the self-attention, the training, and the transformer. It is great resources and easy to understand.
* [Fine-tuning and Masked Language Models](https://web.stanford.edu/~jurafsky/slp3/11.pdf) - Dan Jurafsky and James H. Martin. This chapter is focused on the BERT like transformer and how to fine-tune a model. Both chapters are easy and very insightful