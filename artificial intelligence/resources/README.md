# Additional Resources
## resources on the artfificial intelligence

![tree classifiers](https://github.com/SalvatoreRa/tutorial/blob/main/images/anirudh-YQYacLW8o2U-unsplash.jpg?raw=true)

Photo by [lanirudhreddy](https://unsplash.com/@lanirudhreddy) on [Unsplash](https://unsplash.com/)

&nbsp;

In this section, I will suggest and add many resources on artificial intelligence that can be useful. The list is not exhaustive and I will expand with time, if you want to suggest other resources, you are welcome.

&nbsp;

# **Table of Contents**

* Scientific articles
* Books
* Tools


&nbsp;

# Scientific articles

| Link | Topic | Year | Description |
| --------- | ------ | ------ |------ |
| [LeNet-5](http://yann.lecun.com/exdb/publis/index.html#lecun-98)| computer vision | 1998 |showing that you can stack convolutional layers instead of dense layers |
| [AlexNet ](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)| computer vision | 2012 |first showing the use of ReLu |
| [VGG-16 ](https://arxiv.org/abs/1409.1556)| computer vision | 2014 |first very deep network |
| [Inception-v1 ](https://arxiv.org/abs/1409.4842)| computer vision | 2014 |first models stacking block and not layers (inside a block can be different layers) |
| [Inception-v3 ](https://arxiv.org/abs/1409.1556)| computer vision | 2015 |Among the first designers to use batch normalisation |
| [Res-Net50 ](https://arxiv.org/abs/1512.03385)| computer vision | 2015 |made popular skip connections (resnet block) |
| [Xception](https://arxiv.org/abs/1610.02357)| computer vision | 2016 |depthwise separable convolution layers |
| [Inception-v4 ](https://arxiv.org/abs/1602.07261)| computer vision | 2016 |inception block and residual connection |
| [Inception-ResNet-V2 ](https://arxiv.org/abs/1602.07261)| computer vision | 2016 |Residual Inception blocks |
| [Batch normalization](https://arxiv.org/abs/1502.03167)| computer vision | 2015 |Batch normalization |
| [self attention and convolution](https://arxiv.org/abs/2111.14556)| computer vision | 2021 |On the Integration of Self-Attention and Convolution|
| [Word2vec](https://arxiv.org/pdf/1301.3781v3.pdf)| NLP | 2013 | Word2vec, word embedding |
| [Layer normalization](https://arxiv.org/abs/1607.06450)| NLP | 2013 | layer normalization |
| [RNN ](https://arxiv.org/pdf/1506.02078v1.pdf)| NLP | 2015 |Visualizing and Understanding Recurrent Networks |
| [LSTM ](https://arxiv.org/pdf/1506.02078v1.pdf)| NLP | 2015 |Review about LSTM |
| [GRU ](https://arxiv.org/pdf/1506.02078v1.pdf)| NLP | 2014 |Gated recurrent unit |
| [Transformers ](https://arxiv.org/abs/1706.03762)| NLP | 2016 |First paper proposing transformers |
| [BERT ](https://arxiv.org/abs/1810.04805)| NLP | 2018 | First paper proposing BERT |
| [DistilBERT ](https://arxiv.org/abs/1910.01108)| NLP | 2019 | DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter |
| [Transformers ](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)| NLP | 2018 |GPT-1 paper |
| [Transformers ](https://openai.com/blog/better-language-models/)| NLP | 2018 |GPT-2 paper |
| [Transformers ](https://arxiv.org/abs/2005.14165)| NLP | 2020 |GPT-3 paper |
| [GAN ](https://arxiv.org/pdf/1406.2661v1.pdf)| Generative Learning | 2014 |Generative Adversarial Nets |
| [GAN ](https://arxiv.org/abs/1511.06434)| Generative Learning | 2015 |original article presenting DC-GAN |
| [ADAM](https://jmlr.org/papers/v15/srivastava14a.html)| General interest | 2014 |first paper presenting ADAM |
| [Dropout](https://arxiv.org/abs/1412.6980)| General interest | 2014 |first paper on dropout |
| [Tabular data](https://arxiv.org/abs/2106.03253)| Tabular data | 2021 |Tabular Data: Deep Learning is Not All You Need (showing that deep learning with tabular data is still not the first choice) |
| [Tabular data](https://arxiv.org/pdf/2106.01342.pdf)| Tabular data | 2021 |Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training |
| [Tabular data](https://arxiv.org/pdf/2110.01889.pdf)| Tabular data | 2021 | Method's review on Deep Neural Networks and Tabular Data |
| [Tabular data](https://arxiv.org/pdf/2106.11189.pdf)| Tabular data | 2021 | how regularization impact Neural Network for tabular data|
| [RL](https://arxiv.org/abs/1312.5602)| Reinforcement Leqrning | 2013 | Playing Atari with Deep Reinforcement Learning |

&nbsp;

# Books

| Link | Topic | Year | Description |
| --------- | ------ | ------ |------ |
| [Goodfellow and Bengio](https://www.deeplearningbook.org/) | deep learning | 2016 | One of the most famous book about deep learning|
# Tools

| Link | description | 
| --------- | ------ | 
|  [NN drawer](http://alexlenail.me/NN-SVG/LeNet.html) | Draw online your neural network architechture | 
|  [NN in Latex](http://alexlenail.me/NN-SVG/LeNet.html) | Latex code for drawing neural networks for reports and presentation | 

# Contributing

You can use the issue section or you can write to me. If there are interesting resources, feel free to suggest I will add as soon as I can.

# License

This project is licensed under the **MIT License** 

# Bugs/Issues

Comment or open an issue on Github
